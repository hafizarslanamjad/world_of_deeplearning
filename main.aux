\relax 
\providecommand{\transparent@use}[1]{}
\providecommand \oddpage@label [2]{}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Philosophical Insights of Vector Spaces}{1}{}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Understanding Closure in \(\mathbb  {R}^n\): Why Vector Spaces Are Chosen for Image Representation}{1}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.1}Meaning of Closure}{2}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.2}Why Closure Matters for Images}{2}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.3}How \(\mathbb  {R}^n\) Ensures Closure}{2}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Understanding Column Space and Its Relevance in Deep Learning}{3}{}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Learning Feature Invariance}{5}{}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {paragraph}{Example 1: Sentiment Analysis}{5}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Named Entity Recognition (NER)}{6}{}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{(a) Bag-of-Words (BoW) Embedding.}{8}{}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{(b) Position-Sensitive (Sequential) Embedding.}{8}{}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Image as Matrix}{15}{}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Historical Evolution of Images as Vectors}{15}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}Early Image Digitization (1950s--1970s)}{15}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.2}Pattern Recognition and Statistical Methods (1970s--1990s)}{16}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.3}Deep Learning Era (2000s--Present)}{16}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.4}Summary Timeline}{16}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.2}From Images to Vectors: Justifying Linear Operations in Image Processing}{16}{}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {3.1}{\ignorespaces Timeline of the conceptual development of images as vectors}}{17}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Why Vector Representation is Meaningful}{17}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}Why Vector Operations Make Sense for Images}{18}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.3}Why Linear Algebra is Applicable to Image Processing}{18}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.4}Conclusion}{19}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Pixels, Perception, and the Problem of Meaning}{19}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Vectors as Movements in Space}{20}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Visualizing vector movement, addition, and scalar multiplication.}}{21}{}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:vector_basics}{{3.1}{21}{}{}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.5}The Philosophical Necessity of Linear Transformation in Image Processing}{22}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.6}Matrices Support Transformation}{23}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.7}Controlled Transformations}{26}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.8}The Philosophy of Composition}{27}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.8.1}Why Convolutional Kernels Have Shape}{28}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.9}Images as Structured Tensors}{29}{}\protected@file@percent }
\@writefile{lol}{\contentsline {lstlisting}{\numberline {3.1}{\ignorespaces Creating and passing an image through a Conv2d layer in PyTorch}}{29}{}\protected@file@percent }
\@writefile{lol}{\contentsline {lstlisting}{\numberline {3.2}{\ignorespaces Creating and passing an image through a Conv2D layer in TensorFlow}}{29}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.10}Contextualizing Semantic Meaning}{30}{}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Example of Structural Information.}{30}{}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Example of Syntactic Information.}{31}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.11}The Role of the Convolutional Layer}{32}{}\protected@file@percent }
\@writefile{lol}{\contentsline {lstlisting}{\numberline {3.3}{\ignorespaces Training a convolutional layer: random initialization and learning via gradient descent}}{33}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.12}Emergence of Semantics Through Optimization}{34}{}\protected@file@percent }
\@writefile{lol}{\contentsline {lstlisting}{\numberline {3.4}{\ignorespaces Semantics emerge from loss-driven adaptation rather than explicit supervision}}{34}{}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Common Python Code}{36}{}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Python Data Types and Cheat Sheet}{36}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Typical Function Categories in Python Programming}{37}{}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {4.1}{\ignorespaces Python and PyTorch function cheat sheet with input and output types}}{38}{}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {4.2}{\ignorespaces Summary of common Python object categories}}{38}{}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {4.3}{\ignorespaces Conceptual classification of Python functions}}{42}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.3}The Pattern of Sequence Padding and Length Normalization}{42}{}\protected@file@percent }
\@writefile{lol}{\contentsline {lstlisting}{\numberline {4.1}{\ignorespaces Basic sequence padding pattern}}{43}{}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {4.4}{\ignorespaces Constituents of the sequence padding pattern}}{43}{}\protected@file@percent }
\@writefile{lol}{\contentsline {lstlisting}{\numberline {4.2}{\ignorespaces Truncating a sequence to desired length}}{44}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Building Token Frequencies with \texttt  {collections.Counter}}{44}{}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Motivation.}{44}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.1}What is \texttt  {Counter}?}{45}{}\protected@file@percent }
\@writefile{lol}{\contentsline {lstlisting}{\numberline {4.3}{\ignorespaces Basic Counter example}}{45}{}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Explanation.}{45}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.2}Using \texttt  {Counter.update()}}{45}{}\protected@file@percent }
\@writefile{lol}{\contentsline {lstlisting}{\numberline {4.4}{\ignorespaces Updating a Counter with more data}}{45}{}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Explanation.}{46}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.3}Applying to Tokenized Text Data}{46}{}\protected@file@percent }
\@writefile{lol}{\contentsline {lstlisting}{\numberline {4.5}{\ignorespaces Counting word frequencies across multiple sentences}}{46}{}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Explanation.}{46}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.5}Understanding \texttt  {torch.zeros()} in PyTorch}{47}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.1}Function Signature}{47}{}\protected@file@percent }
\@writefile{lol}{\contentsline {lstlisting}{\numberline {4.6}{\ignorespaces Create a 1D tensor with 5 zeros}}{47}{}\protected@file@percent }
\@writefile{lol}{\contentsline {lstlisting}{\numberline {4.7}{\ignorespaces Create a 2D tensor with shape (3, 4)}}{48}{}\protected@file@percent }
\@writefile{lol}{\contentsline {lstlisting}{\numberline {4.8}{\ignorespaces Using dtype and device parameters}}{48}{}\protected@file@percent }
\@writefile{lol}{\contentsline {lstlisting}{\numberline {4.9}{\ignorespaces Tensor that supports autograd}}{48}{}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Common Use Cases}{48}{}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Spatially Local Pattern}{49}{}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Subtraction as a Fundamental Tool for Change Detection}{50}{}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}Introduction}{50}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6.2}Mathematical and Logical Basis of Subtraction}{50}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6.3}Subtraction in Temporal and Logical Structures}{51}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6.4}Subtraction in Visual and Spatial Domains}{52}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6.5}Geometric Reasoning and Directionality of Change}{52}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6.6}Subtraction in Natural Language Processing}{53}{}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{1. What do we mean by \textquotedblleft semantic\textquotedblright ?}{55}{}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{2. What is an embedding?}{55}{}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{3. What does the subtraction $A - B$ mean in this context?}{55}{}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{4. Where does this semantic encoding come from?}{56}{}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{5. Summary Table}{56}{}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {6.1}{\ignorespaces Summary of semantic vector reasoning}}{57}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6.7}Conclusion}{57}{}\protected@file@percent }
\gdef \@abspage@last{63}
