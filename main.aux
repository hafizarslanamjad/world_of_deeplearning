\relax 
\providecommand{\transparent@use}[1]{}
\providecommand \oddpage@label [2]{}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Philosophical Insights of Vector Spaces}{1}{}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Understanding Closure in \(\mathbb  {R}^n\): Why Vector Spaces Are Chosen for Image Representation}{1}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.1}Meaning of Closure}{2}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.2}Why Closure Matters for Images}{2}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.3}How \(\mathbb  {R}^n\) Ensures Closure}{2}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Understanding Column Space and Its Relevance in Deep Learning}{3}{}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Natural Language Processing}{5}{}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}\texttt  {vectorizer.fit\_transform}}{5}{}\protected@file@percent }
\@writefile{lol}{\contentsline {lstlisting}{\numberline {2.1}{\ignorespaces Learning the vocabulary with \texttt  {fit}}}{5}{}\protected@file@percent }
\@writefile{lol}{\contentsline {lstlisting}{\numberline {2.2}{\ignorespaces Vocabulary learned by \texttt  {fit}}}{5}{}\protected@file@percent }
\@writefile{lol}{\contentsline {lstlisting}{\numberline {2.3}{\ignorespaces Transforming a new document}}{6}{}\protected@file@percent }
\@writefile{lol}{\contentsline {lstlisting}{\numberline {2.4}{\ignorespaces Vector representation}}{6}{}\protected@file@percent }
\@writefile{lol}{\contentsline {lstlisting}{\numberline {2.5}{\ignorespaces Using fit\_transform directly}}{6}{}\protected@file@percent }
\@writefile{lol}{\contentsline {lstlisting}{\numberline {2.6}{\ignorespaces Matrix representation of the corpus}}{6}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Mathematical Representation}{6}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Term Frequency (TF)}{7}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Why the Logarithm in IDF?}{10}{}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Learning Feature Invariance}{12}{}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {paragraph}{Example 1: Sentiment Analysis}{12}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Named Entity Recognition (NER)}{13}{}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{(a) Bag-of-Words (BoW) Embedding.}{15}{}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{(b) Position-Sensitive (Sequential) Embedding.}{15}{}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Image as Matrix}{22}{}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Historical Evolution of Images as Vectors}{22}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.1}Early Image Digitization (1950s--1970s)}{22}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.2}Pattern Recognition and Statistical Methods (1970s--1990s)}{23}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.3}Deep Learning Era (2000s--Present)}{23}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.4}Summary Timeline}{23}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.2}From Images to Vectors: Justifying Linear Operations in Image Processing}{23}{}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {4.1}{\ignorespaces Timeline of the conceptual development of images as vectors}}{24}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}Why Vector Representation is Meaningful}{24}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.2}Why Vector Operations Make Sense for Images}{25}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.3}Why Linear Algebra is Applicable to Image Processing}{25}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.4}Conclusion}{26}{}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Image as Matrix}{27}{}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Pixels, Perception, and the Problem of Meaning}{27}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Vectors as Movements in Space}{28}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces Visualizing vector movement, addition, and scalar multiplication.}}{30}{}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:vector_basics}{{5.1}{30}{}{}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.3}The Philosophical Necessity of Linear Transformation in Image Processing}{30}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.4}Matrices Support Transformation}{31}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.5}Controlled Transformations}{34}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.6}The Philosophy of Composition}{35}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.6.1}Why Convolutional Kernels Have Shape}{36}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.7}Images as Structured Tensors}{37}{}\protected@file@percent }
\@writefile{lol}{\contentsline {lstlisting}{\numberline {5.1}{\ignorespaces Creating and passing an image through a Conv2d layer in PyTorch}}{37}{}\protected@file@percent }
\@writefile{lol}{\contentsline {lstlisting}{\numberline {5.2}{\ignorespaces Creating and passing an image through a Conv2D layer in TensorFlow}}{38}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.8}Contextualizing Semantic Meaning}{38}{}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Example of Structural Information.}{39}{}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Example of Syntactic Information.}{39}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.9}The Role of the Convolutional Layer}{40}{}\protected@file@percent }
\@writefile{lol}{\contentsline {lstlisting}{\numberline {5.3}{\ignorespaces Training a convolutional layer: random initialization and learning via gradient descent}}{41}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.10}Emergence of Semantics Through Optimization}{42}{}\protected@file@percent }
\@writefile{lol}{\contentsline {lstlisting}{\numberline {5.4}{\ignorespaces Semantics emerge from loss-driven adaptation rather than explicit supervision}}{43}{}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Common Python Code}{45}{}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}Python Data Types and Cheat Sheet}{45}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6.2}Typical Function Categories in Python Programming}{46}{}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {6.1}{\ignorespaces Python and PyTorch function cheat sheet with input and output types}}{47}{}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {6.2}{\ignorespaces Summary of common Python object categories}}{47}{}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {6.3}{\ignorespaces Conceptual classification of Python functions}}{51}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6.3}The Pattern of Sequence Padding and Length Normalization}{51}{}\protected@file@percent }
\@writefile{lol}{\contentsline {lstlisting}{\numberline {6.1}{\ignorespaces Basic sequence padding pattern}}{52}{}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {6.4}{\ignorespaces Constituents of the sequence padding pattern}}{52}{}\protected@file@percent }
\@writefile{lol}{\contentsline {lstlisting}{\numberline {6.2}{\ignorespaces Truncating a sequence to desired length}}{53}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6.4}Building Token Frequencies with \texttt  {collections.Counter}}{53}{}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Motivation.}{53}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4.1}What is \texttt  {Counter}?}{54}{}\protected@file@percent }
\@writefile{lol}{\contentsline {lstlisting}{\numberline {6.3}{\ignorespaces Basic Counter example}}{54}{}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Explanation.}{54}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4.2}Using \texttt  {Counter.update()}}{54}{}\protected@file@percent }
\@writefile{lol}{\contentsline {lstlisting}{\numberline {6.4}{\ignorespaces Updating a Counter with more data}}{54}{}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Explanation.}{55}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4.3}Applying to Tokenized Text Data}{55}{}\protected@file@percent }
\@writefile{lol}{\contentsline {lstlisting}{\numberline {6.5}{\ignorespaces Counting word frequencies across multiple sentences}}{55}{}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Explanation.}{55}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6.5}Understanding \texttt  {torch.zeros()} in PyTorch}{56}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.5.1}Function Signature}{56}{}\protected@file@percent }
\@writefile{lol}{\contentsline {lstlisting}{\numberline {6.6}{\ignorespaces Create a 1D tensor with 5 zeros}}{56}{}\protected@file@percent }
\@writefile{lol}{\contentsline {lstlisting}{\numberline {6.7}{\ignorespaces Create a 2D tensor with shape (3, 4)}}{57}{}\protected@file@percent }
\@writefile{lol}{\contentsline {lstlisting}{\numberline {6.8}{\ignorespaces Using dtype and device parameters}}{57}{}\protected@file@percent }
\@writefile{lol}{\contentsline {lstlisting}{\numberline {6.9}{\ignorespaces Tensor that supports autograd}}{57}{}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Common Use Cases}{57}{}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {7}Spatially Local Pattern}{58}{}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {chapter}{\numberline {8}Subtraction as a Fundamental Tool for Change Detection}{59}{}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {8.1}Introduction}{59}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {8.2}Mathematical and Logical Basis of Subtraction}{59}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {8.3}Subtraction in Temporal and Logical Structures}{60}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {8.4}Subtraction in Visual and Spatial Domains}{61}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {8.5}Geometric Reasoning and Directionality of Change}{61}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {8.6}Subtraction in Natural Language Processing}{62}{}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{1. What do we mean by \textquotedblleft semantic\textquotedblright ?}{64}{}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{2. What is an embedding?}{64}{}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{3. What does the subtraction $A - B$ mean in this context?}{64}{}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{4. Where does this semantic encoding come from?}{65}{}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{5. Summary Table}{65}{}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {8.1}{\ignorespaces Summary of semantic vector reasoning}}{66}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {8.7}Conclusion}{66}{}\protected@file@percent }
\gdef \@abspage@last{72}
