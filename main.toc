\contentsline {chapter}{\numberline {1}Philosophical Insights of Vector Spaces}{1}{}%
\contentsline {section}{\numberline {1.1}Understanding Closure in \(\mathbb {R}^n\): Why Vector Spaces Are Chosen for Image Representation}{1}{}%
\contentsline {subsection}{\numberline {1.1.1}Meaning of Closure}{2}{}%
\contentsline {subsection}{\numberline {1.1.2}Why Closure Matters for Images}{2}{}%
\contentsline {subsection}{\numberline {1.1.3}How \(\mathbb {R}^n\) Ensures Closure}{2}{}%
\contentsline {section}{\numberline {1.2}Understanding Column Space and Its Relevance in Deep Learning}{3}{}%
\contentsline {chapter}{\numberline {2}Natural Language Processing}{5}{}%
\contentsline {section}{\numberline {2.1}\texttt {vectorizer.fit\_transform}}{5}{}%
\contentsline {subsection}{\numberline {2.1.1}Mathematical Representation}{6}{}%
\contentsline {section}{\numberline {2.2}Term Frequency (TF)}{7}{}%
\contentsline {section}{\numberline {2.3}Why the Logarithm in IDF?}{10}{}%
\contentsline {chapter}{\numberline {3}Learning Feature Invariance}{12}{}%
\contentsline {paragraph}{Example 1: Sentiment Analysis}{12}{}%
\contentsline {section}{\numberline {3.1}Named Entity Recognition (NER)}{13}{}%
\contentsline {paragraph}{(a) Bag-of-Words (BoW) Embedding.}{15}{}%
\contentsline {paragraph}{(b) Position-Sensitive (Sequential) Embedding.}{15}{}%
\contentsline {chapter}{\numberline {4}Image as Matrix}{22}{}%
\contentsline {section}{\numberline {4.1}Historical Evolution of Images as Vectors}{22}{}%
\contentsline {subsection}{\numberline {4.1.1}Early Image Digitization (1950s--1970s)}{22}{}%
\contentsline {subsection}{\numberline {4.1.2}Pattern Recognition and Statistical Methods (1970s--1990s)}{23}{}%
\contentsline {subsection}{\numberline {4.1.3}Deep Learning Era (2000s--Present)}{23}{}%
\contentsline {subsection}{\numberline {4.1.4}Summary Timeline}{23}{}%
\contentsline {section}{\numberline {4.2}From Images to Vectors: Justifying Linear Operations in Image Processing}{23}{}%
\contentsline {subsection}{\numberline {4.2.1}Why Vector Representation is Meaningful}{24}{}%
\contentsline {subsection}{\numberline {4.2.2}Why Vector Operations Make Sense for Images}{25}{}%
\contentsline {subsection}{\numberline {4.2.3}Why Linear Algebra is Applicable to Image Processing}{25}{}%
\contentsline {subsection}{\numberline {4.2.4}Conclusion}{26}{}%
\contentsline {chapter}{\numberline {5}Image as Matrix}{27}{}%
\contentsline {section}{\numberline {5.1}Pixels, Perception, and the Problem of Meaning}{27}{}%
\contentsline {section}{\numberline {5.2}Vectors as Movements in Space}{28}{}%
\contentsline {section}{\numberline {5.3}The Philosophical Necessity of Linear Transformation in Image Processing}{30}{}%
\contentsline {section}{\numberline {5.4}Matrices Support Transformation}{31}{}%
\contentsline {section}{\numberline {5.5}Controlled Transformations}{34}{}%
\contentsline {section}{\numberline {5.6}The Philosophy of Composition}{35}{}%
\contentsline {subsection}{\numberline {5.6.1}Why Convolutional Kernels Have Shape}{36}{}%
\contentsline {section}{\numberline {5.7}Images as Structured Tensors}{37}{}%
\contentsline {section}{\numberline {5.8}Contextualizing Semantic Meaning}{38}{}%
\contentsline {paragraph}{Example of Structural Information.}{39}{}%
\contentsline {paragraph}{Example of Syntactic Information.}{39}{}%
\contentsline {section}{\numberline {5.9}The Role of the Convolutional Layer}{40}{}%
\contentsline {section}{\numberline {5.10}Emergence of Semantics Through Optimization}{42}{}%
\contentsline {chapter}{\numberline {6}Common Python Code}{45}{}%
\contentsline {section}{\numberline {6.1}Python Data Types and Cheat Sheet}{45}{}%
\contentsline {section}{\numberline {6.2}Typical Function Categories in Python Programming}{46}{}%
\contentsline {section}{\numberline {6.3}The Pattern of Sequence Padding and Length Normalization}{51}{}%
\contentsline {section}{\numberline {6.4}Building Token Frequencies with \texttt {collections.Counter}}{53}{}%
\contentsline {paragraph}{Motivation.}{53}{}%
\contentsline {subsection}{\numberline {6.4.1}What is \texttt {Counter}?}{54}{}%
\contentsline {paragraph}{Explanation.}{54}{}%
\contentsline {subsection}{\numberline {6.4.2}Using \texttt {Counter.update()}}{54}{}%
\contentsline {paragraph}{Explanation.}{55}{}%
\contentsline {subsection}{\numberline {6.4.3}Applying to Tokenized Text Data}{55}{}%
\contentsline {paragraph}{Explanation.}{55}{}%
\contentsline {section}{\numberline {6.5}Understanding \texttt {torch.zeros()} in PyTorch}{56}{}%
\contentsline {subsection}{\numberline {6.5.1}Function Signature}{56}{}%
\contentsline {paragraph}{Common Use Cases}{57}{}%
\contentsline {chapter}{\numberline {7}Spatially Local Pattern}{58}{}%
\contentsline {chapter}{\numberline {8}Subtraction as a Fundamental Tool for Change Detection}{59}{}%
\contentsline {section}{\numberline {8.1}Introduction}{59}{}%
\contentsline {section}{\numberline {8.2}Mathematical and Logical Basis of Subtraction}{59}{}%
\contentsline {section}{\numberline {8.3}Subtraction in Temporal and Logical Structures}{60}{}%
\contentsline {section}{\numberline {8.4}Subtraction in Visual and Spatial Domains}{61}{}%
\contentsline {section}{\numberline {8.5}Geometric Reasoning and Directionality of Change}{61}{}%
\contentsline {section}{\numberline {8.6}Subtraction in Natural Language Processing}{62}{}%
\contentsline {paragraph}{1. What do we mean by \textquotedblleft semantic\textquotedblright ?}{64}{}%
\contentsline {paragraph}{2. What is an embedding?}{64}{}%
\contentsline {paragraph}{3. What does the subtraction $A - B$ mean in this context?}{64}{}%
\contentsline {paragraph}{4. Where does this semantic encoding come from?}{65}{}%
\contentsline {paragraph}{5. Summary Table}{65}{}%
\contentsline {section}{\numberline {8.7}Conclusion}{66}{}%
